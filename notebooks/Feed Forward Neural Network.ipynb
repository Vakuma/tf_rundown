{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "## Feed forward Neural Networks (MLP)\n",
    "---\n",
    "\n",
    "Lets load the MNIST dataset first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Necessary imports\n",
    "import time\n",
    "from IPython import display\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image, ImageOps\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Read the mnist dataset\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Our feed forward neural network will look very similar to our softmax classifier. However, now we have multiple layers and non-linear activations over logits!\n",
    "\n",
    "For this network, we will have 3 layers. 2 hidden layers and 1 output layer. The output layer is a softmax classifier that we implemented in the previous example.\n",
    "\n",
    "Like in the previous example, let's first define our hyperparameters which now include layer sizes  \n",
    "*Apparently layer sizes are usually in [powers of 2](http://rishy.github.io/ml/2017/01/05/how-to-train-your-dnn/) for computational reasons*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters (these are similar to the ones used in the previous example)\n",
    "learning_rate = 0.1\n",
    "training_epochs = 5\n",
    "batch_size = 100\n",
    "\n",
    "# Additional hyperparameters for our Neural Nets - Layer sizes\n",
    "layer_1_size = 256\n",
    "layer_2_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* **Step 1**: Create placeholders to hold the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create placeholders\n",
    "x_tensor = tf.placeholder(tf.float32, shape=(None, 784))\n",
    "y_tensor = tf.placeholder(tf.float32, shape=(None, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* **Step 2**: Create variables to hold the weight matrices and the bias vectors for all the layers\n",
    "\n",
    "  Note that the weights are now initialized with small random numbers. From [Karpathy](http://cs231n.github.io/neural-networks-2/):\n",
    "  > \"A reasonable-sounding idea then might be to set all the initial weights to zero, which we expect to be the “best guess” in expectation. This turns out to be a mistake, because if every neuron in the network computes the same output, then they will also all compute the same gradients during backpropagation and undergo the exact same parameter updates. In other words, there is no source of asymmetry between neurons if their weights are initialized to be the same.\"\n",
    "  \n",
    "  > \"the implementation for one weight matrix might look like `W = 0.01* np.random.randn()`, where randn samples from a zero mean, unit standard deviation gaussian.\"\n",
    "  \n",
    "  Suggested rule of thumb\n",
    "  > \"Initialize the weights by drawing them from a gaussian distribution with standard deviation of sqrt(2/n), where n is the number of inputs to the neuron. E.g. in numpy: `W = np.random.randn(n) * sqrt(2.0/n).`\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Model parameters that have to be learned\n",
    "\n",
    "# Note that the weights & biases are now initialized to small random numbers\n",
    "# Also note that the number of columns for should be the size of the first layer!\n",
    "W_h1 = tf.Variable(0.01 * tf.random_normal([784, layer_1_size]))\n",
    "b_h1 = tf.Variable(tf.random_normal([layer_1_size]))\n",
    "\n",
    "# Layer 2\n",
    "# The input dimensions are not 784 anymore but the size of the first layer. \n",
    "# The number of columns are the size of the second layer\n",
    "W_h2 = tf.Variable(0.01 * tf.random_normal([layer_1_size, layer_2_size]))\n",
    "b_h2 = tf.Variable(tf.random_normal([layer_2_size]))\n",
    "\n",
    "# Output layer - Layer 3\n",
    "# This is the softmax layer that we implemented earlier\n",
    "# The input dimension size is now the size of the 2nd layer and the number of columns = number of classes\n",
    "W_o = tf.Variable(0.01 * tf.random_normal([layer_2_size, 10]))\n",
    "b_o = tf.Variable(tf.random_normal([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* **Step 3**: Lets build the flow of data. Each unit in each layer computes the logits (Linear function = W * X + b). Next, it applies an activation function over each of the logits and passes them on as inputs to the next layer.\n",
    "  \n",
    "  \n",
    "* **Step 4**: Compute the loss function as the cross entropy between the predicted distribution of the labels from the output layer and its true distribution.  \n",
    "  Note that we will now simply use the tensor flow's softmax compute entropy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get the logits for the first layer\n",
    "logits_h1 = tf.matmul(x_tensor, W_h1) + b_h1\n",
    "# Compute the activations which forms the output of this layer\n",
    "out_h1 = tf.sigmoid(logits_h1)\n",
    "# out_h1 = tf.nn.relu(logits_h1)\n",
    "\n",
    "# Get the logits for the second layer\n",
    "# Note that the input is now the output from the previous layer\n",
    "logits_h2 = tf.matmul(out_h1, W_h2) + b_h2\n",
    "# Compute the activations which forms the output of this layer\n",
    "out_h2 = tf.sigmoid(logits_h2)\n",
    "# out_h2 = tf.nn.relu(logits_h2)\n",
    "\n",
    "# Get the logits for the output layer\n",
    "logits_o = tf.matmul(out_h2, W_o) + b_o\n",
    "\n",
    "# Final layer doesn't have activations. Simply compute the cross entropy loss\n",
    "cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_tensor, logits=logits_o)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* **Step 5**: Lets create an optimizer to minimize the cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create an optimizer with the learning rate\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "# Use the optimizer to minimize the loss\n",
    "train_step = optimizer.minimize(cross_entropy_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* **Step 6**: Lets compute the accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# First create the correct prediction by taking the maximum value from the prediction class\n",
    "# and checking it with the actual class. The result is a boolean column vector\n",
    "correct_predictions = tf.equal(tf.argmax(logits_o, 1), tf.argmax(y_tensor, 1))\n",
    "# Calculate the accuracy over all the images\n",
    "# Cast the boolean vector into float (1s & 0s) and then compute the average. \n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now lets run our graph as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/5. Batch: 500/550. Current loss: 1.60759. Train Accuracy: 0.35\n",
      "Test Accuracy: 0.35\n"
     ]
    }
   ],
   "source": [
    "# Initializing global variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Create a saver to save our model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Create a session to run the graph\n",
    "with tf.Session() as sess:\n",
    "    # Run initialization\n",
    "    sess.run(init)\n",
    "\n",
    "    # For the set number of epochs\n",
    "    for epoch in range(training_epochs):\n",
    "        \n",
    "        # Compute the total number of batches\n",
    "        num_batches = int(mnist.train.num_examples/batch_size)\n",
    "        \n",
    "        # Iterate over all the examples (1 epoch)\n",
    "        for batch in range(num_batches):\n",
    "            \n",
    "            # Get a batch of examples\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # Now run the session \n",
    "            curr_loss, cur_accuracy, _ = sess.run([cross_entropy_loss, accuracy, train_step], \n",
    "                                                    feed_dict={x_tensor: batch_xs, y_tensor: batch_ys})\n",
    "            \n",
    "            if batch % 50 == 0:\n",
    "                display.clear_output(wait=True)\n",
    "                time.sleep(0.05)\n",
    "                # Print the loss\n",
    "                print(\"Epoch: %d/%d. Batch: %d/%d. Current loss: %.5f. Train Accuracy: %.2f\"\n",
    "                      %(epoch, training_epochs, batch, num_batches, curr_loss, cur_accuracy))\n",
    "            \n",
    "    # Run the session to compute the value and print it\n",
    "    test_accuracy = sess.run(accuracy,\n",
    "                                       feed_dict={x_tensor: mnist.test.images, \n",
    "                                                  y_tensor: mnist.test.labels})\n",
    "    print(\"Test Accuracy: %.2f\"%test_accuracy)\n",
    "    \n",
    "    # Lets save the entire session\n",
    "    saver.save(sess, '../models/ff_nn.model')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.35\n"
     ]
    }
   ],
   "source": [
    "# Load the model back and test its accuracy\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, '../models/ff_nn.model')\n",
    "    test_accuracy = sess.run(accuracy,\n",
    "                                       feed_dict={x_tensor: mnist.test.images, \n",
    "                                                  y_tensor: mnist.test.labels})\n",
    "    print(\"Test Accuracy: %.2f\"%test_accuracy)       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
