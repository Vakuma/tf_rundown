{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "## Classification - MNIST dataset\n",
    "---\n",
    "\n",
    "Exploring the popular MNIST dataset. \n",
    "\n",
    "Tensorflow provides a function to ingest the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Necessary imports\n",
    "import time\n",
    "from IPython import display\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image, ImageOps\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Read the mnist dataset\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of MNIST Images.\n",
      "Shape = (num_examples * num_features/pixels)\n",
      "\n",
      "Train      :  (55000, 784)\n",
      "Validation :  (5000, 784)\n",
      "Train      :  (10000, 784)\n",
      "-------------------------\n",
      "Shape of MNIST Labels.\n",
      "Shape = (num_examples * num_labels/classes)\n",
      "\n",
      "Train      :  (55000, 10)\n",
      "Validation :  (5000, 10)\n",
      "Train      :  (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Explore mnist\n",
    "print(\"Shape of MNIST Images.\\nShape = (num_examples * num_features/pixels)\\n\")\n",
    "print(\"Train      : \", mnist.train.images.shape)\n",
    "print(\"Validation : \", mnist.validation.images.shape)\n",
    "print(\"Train      : \", mnist.test.images.shape)\n",
    "print(\"-\"*25)\n",
    "print(\"Shape of MNIST Labels.\\nShape = (num_examples * num_labels/classes)\\n\")\n",
    "print(\"Train      : \", mnist.train.labels.shape)\n",
    "print(\"Validation : \", mnist.validation.labels.shape)\n",
    "print(\"Train      : \", mnist.test.labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Lets look at a random image and its label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image label: 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fcceaf47d68>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcd543962b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pull out a random image & its label\n",
    "random_image_index = 200\n",
    "random_image = mnist.train.images[random_image_index]\n",
    "random_image_label = mnist.train.labels[random_image_index]\n",
    "\n",
    "# Print the label and the image as grayscale\n",
    "print(\"Image label: %d\"%(random_image_label.argmax()))\n",
    "pil_image = Image.fromarray(((random_image.reshape(28,28)) * 256).astype('uint8'), \"L\")\n",
    "imshow(ImageOps.invert(pil_image), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "## Logistic Regression - Softmax\n",
    "---\n",
    "\n",
    "Now let's build a softmax classifier (linear) to classify MNIST images. We will use Mini-batch gradient descent for optimization\n",
    "\n",
    "First, declare some of the hyperparameters that will be used by our softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Softmax hyperparameters\n",
    "learning_rate = 0.5\n",
    "training_epochs = 5\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* **Step 1**: Create placeholders to hold the images. \n",
    "\n",
    "  Using `None` for a dimension in shape means it can be *any* number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create placeholders\n",
    "x = tf.placeholder(tf.float32, shape=(None, 784))\n",
    "y = tf.placeholder(tf.float32, shape=(None, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* **Step 2**: Create variables to hold the weight matrix and the bias vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Model parameters that have to be learned\n",
    "# Initialize with zeros\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* **Step 3**: Lets compute the label distribution. Apply the linear function W * X + b for each of the 10 classes. Then apply the softmax function to get a probability distribution of likelihoods for classes.  \n",
    "  Recall that softmax(x) = exp(x)/ sum_i(exp(i)) where i represents each class\n",
    "  \n",
    "  \n",
    "* **Step 4**: Compute the loss function as the cross entropy between the predicted distribution of the labels and its true distribution.  \n",
    "  Cross entropy H(Y) = - sum_i( true_dist(i) * log (computed_dist(i)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get all the logits i.e. W * X + b for each of the class\n",
    "logits = tf.matmul(x, W) + b\n",
    "# Take a softmax of the logits. \n",
    "y_predicted = tf.nn.softmax(logits)\n",
    "\n",
    "# Make sure you reduce the sum across columns.\n",
    "# The y_predicted has a shape of number_of_examples * 10\n",
    "# Cross entropy should first sum across columns to get individual cost and then average this error over all examples\n",
    "cross_entropy_loss = tf.reduce_mean(- tf.reduce_sum(y * tf.log(y_predicted ), axis=1))\n",
    "\n",
    "# This can apparently be numerically unstable. \n",
    "# Tensorflow provides a function that computes the logits, applies softmax and computes the cross entropy\n",
    "# The example above is split only for pedagogical purposes\n",
    "# logits = tf.matmul(x, W) + b\n",
    "# cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* **Step 5**: Lets create a gradient descent optimizer to minimize the cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create an optimizer with the learning rate\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "# Use the optimizer to minimize the loss\n",
    "train_step = optimizer.minimize(cross_entropy_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* **Step 6**: Lets compute the accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# First create the correct prediction by taking the maximum value from the prediction class\n",
    "# and checking it with the actual class. The result is a boolean column vector\n",
    "correct_predictions = tf.equal(tf.argmax(y_predicted, 1), tf.argmax(y, 1))\n",
    "# Calculate the accuracy over all the images\n",
    "# Cast the boolean vector into float (1s & 0s) and then compute the average. \n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now lets run our graph as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/5. Batch #: 500/550. Current loss: 0.27087. Train Accuracy: 0.92\n",
      "Test Accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "# Initializing global variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Create a session to run the graph\n",
    "with tf.Session() as sess:\n",
    "    # Run initialization\n",
    "    sess.run(init)\n",
    "\n",
    "    # For the set number of epochs\n",
    "    for epoch in range(training_epochs):\n",
    "        \n",
    "        # Compute the total number of batches\n",
    "        num_batches = int(mnist.train.num_examples/batch_size)\n",
    "        \n",
    "        # Iterate over all the examples (1 epoch)\n",
    "        for batch_num in range(num_batches):\n",
    "            \n",
    "            # Get a batch of examples\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # Now run the session \n",
    "            curr_loss, cur_accuracy, _ = sess.run([cross_entropy_loss, accuracy, train_step], \n",
    "                                                    feed_dict={x: batch_xs, y: batch_ys})\n",
    "            \n",
    "            if batch_num % 50 == 0:\n",
    "                display.clear_output(wait=True)\n",
    "                time.sleep(0.1)\n",
    "                # Print the loss\n",
    "                print(\"Epoch: %d/%d. Batch #: %d/%d. Current loss: %.5f. Train Accuracy: %.2f\"\n",
    "                      %(epoch, training_epochs, batch_num, num_batches, curr_loss, cur_accuracy))\n",
    "\n",
    "    # Run the session to compute the value and print it\n",
    "    test_accuracy = sess.run(accuracy,\n",
    "                                       feed_dict={x: mnist.test.images, \n",
    "                                                  y: mnist.test.labels})\n",
    "    print(\"Test Accuracy: %.2f\"%test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
